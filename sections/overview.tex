\section{Offline Computing Model Overview}
\label{sec:overview}

%The Mu2e Offline Computing Model describes the software, infrastructure, tools, and workflows to store, process, simulate, and analyze data. Offline covers all computing activities except data acquisition; Online computing and the data acquisition system are not included in this scope, but a summary is provided in section~\ref{sec:daq} to introduce the concepts and terminology used in the rest of this document. The formal interface between Online and Offline lies at the DAQ buffer disks in the Mu2e building. Online is responsible for writing data on these disks, and Offline is in charge of transferring the files for further processing (and safely deleting them on the buffer disk). 

%The Offline Computing Model is based on the framework and tools provided by the FNAL scientific computing division (SCD), the Fabric for Intensity Frontier Experiments (FIFE) group, and global infrastructures such as the Open Science Grid (OSG) or High-Performance Computing (HPC) centers. The event processing framework is built on the {\it art} framework developed by SCD, supplemented by Mu2e-specific algorithms, a condition system, a geometry description, and the definitions of data products. All calibration, reconstruction, and simulation algorithms operate within this environment. This code base is usually referred to as Mu2e Offline (or just Offline). This model has been in development for more than a decade, from characterizing the detector performance in the TDR and CD-3 phases to the most recent estimate of the experiment sensitivity~\cite{Mu2e:2022ggl}. At each stage, the model has evolved to manage the growing processing requirements and the increasing complexity of the simulation and reconstruction tasks. This model will continue to mature to take advantage of future computing technologies and infrastructures as well as advances in computational techniques over the coming decade. 

The Mu2e Offline Computing Model (OCM) describes the software, infrastructure, tools, and workflows to store, process, simulate, and analyze data. This definition covers all computing activities downstream of the data acquisition process. The DAQ system and Online software are not included in this scope, but a summary is provided in section~\ref{sec:daq} to introduce the concepts and terminology used in the rest of this document. The formal interface between Online and Offline lies at the DAQ buffer disks in the Mu2e building. Online is responsible for writing data on these disks, and Offline is in charge of transferring the files for further processing and safely deleting them on the buffer disk. 

%Mu2e is a medium-scale collaboration with moderate computing needs compared to large-scale experiments, but also a lower level of resources. 

The OCM is devised to support Mu2e physics goals by designing systems minimizing operations effort, addressing anticipated obstacles, and aligning with the host laboratory tools, support, and strategy. This system is based on the framework and tools provided by the FNAL Computing Science and Artificial Intelligence Division (CSAID) and the Fabric for Intensity Frontier Experiments (FIFE) group for small- and mid-scale intensity frontier experiments. The computing resources are mostly provided by the host laboratory in the form of conventional UNIX batch systems accessible via the Open Science Grid (OSG), supplemented by external resources whenever available (e.g. high-performance computing centers). However, several hurdles limit the use of external resources. They are usually not controlled by Mu2e, or even HEP, and might be subject to annual applications and restrictions. Some centers may also have limited network connectivity, making it difficult to move large amounts of data in a reliable manner. In these instances, performing computing tasks with large CPU/IO ratios (e.g. simulation) may be more advantageous. Finally, High Performance Computing Centers (HPCs) are very diverse in the hardware offered and might require significant software development effort to use specific resources efficiently. The overall Mu2e computing strategy is, therefore, to store all raw, reconstructed, and simulated data at the host laboratory to perform most of the simulation, reconstruction, and calibration tasks. External resources are exploited whenever available to perform non-critical tasks, such as simulation or data reprocessing, privileging CPU-intensive operations. Reduced data sets of modest size are also produced to enable data analysis at local institutions. 

One of the main components of the OCM, the event processing framework, is built on the {\it art} framework developed by SCD, supplemented by Mu2e-specific algorithms, a condition system, a geometry description, and the definitions of data products. This code base is usually referred to as Mu2e Offline (or just Offline). This model has been in development for more than a decade, from characterizing the detector performance in the TDR and CD-3 phases to the most recent estimate of the experiment sensitivity~\cite{Mu2e:2022ggl}. At each stage, the model has evolved to manage the growing processing requirements and the increasing complexity of the simulation and reconstruction tasks. This model will continue to mature to take advantage of future computing technologies and infrastructures as well as advances in computational techniques over the coming decade. 

The workflow designed for the OCM starts with the data written by the DAQ system on the buffer disk. At the beginning of each run, the DAQ system copies all information used to configure the detector and DAQ into the online run conditions database. The online content is periodically streamed to an offline database instance. As new runs appear in the online database, the raw data are transferred from the DAQ storage disk to a disk visible to Offline worker nodes. There are about 15 independent data streams but these may be packaged into fewer files due to constraints from the TDAQ system and data movement capabilities. A first pass (\passone) of Offline reconstruction is triggered with minimal latency. This \passone\ uses the reconstruction results to produce updated calibration constants and offline data quality metrics, persisted in the corresponding databases. The data are then reprocessed (\passtwo) with the updated calibration conditions. Reconstructed data objects from both passes are stored to tape and registered in a file catalog. A blinding scheme will be developed and applied to data released to collaborators to avoid any implicit experimental bias. In specific cases, e.g. if substantial improvements to either the reconstruction codes or calibration values, a complete data reprocessing of all data will be done. Finally, reduced data sets (aka skims and ntuples) are produced for further analysis. This workflow assumes 8/5 support for offline databases, with best effort outside standard hours.

Mu2e events are selected by trigger algorithms based mostly on information from the tracker and calorimeter. The low-level digital data coming from each subsystem are first converted into physical objects using calibration data (e.g. objects providing a physical time, position, energy, etc.). A set of algorithms then filters and aggregates these hits into increasingly complete physical objects. A two-step clustering algorithm is used to form calorimeter clusters from crystal hits, including a procedure to recover split-off clusters. The track-finding algorithm starts by filtering hits produced by low-energy electrons with dedicated algorithms, then identifies clusters of hits with a short time window (typically $\sim$50 ns). These hits are passed to helix finding algorithms to extract an approximate helix parameterization. One algorithm uses the position of high-energy calorimeter clusters and the stopping target to seed the helix search, while the second is purely based on tracker information. The calorimeter-seeded algorithm tends to be more robust against higher levels of background but has inherently lower efficiency. Reconstructed helices are finally passed to a Kalman filter track fitter. Two configurations have been developed for the Mu2e KinKal fit, one optimized for use in the online trigger, and a second for offline analysis. Physics and calibration data are selected by a set of trigger filters based on the reconstructed tracks and calorimeter clusters, with adjustable prescale factors to tune the total trigger rate. Data from the extinction monitor and stopping target monitor are reconstructed in dedicated applications. Their final outputs are associated with the corresponding event data by their DAQ timestamp. 

Complete end-to-end simulations of Mu2e datasets are prohibitively expensive to produce directly given the huge number of protons on target. Current samples were produced by splitting the processing into pileup, signals, and physics backgrounds. Geant4 is used to model the Mu2e experiment and particle interactions. Pileup is simulated starting from protons hitting the production target, recording the proton daughter particles as they exit the Transport Solenoid. Daughter particles are re-simulated (resampled) many times to increase the effective statistics, limited by the eventual repetition of the same daughter particles (oversampling). Current pileup samples were produced with a $\sim 10$M core hour campaign, producing roughly 1 second of pileup at the expected nominal Mu2e intensity. Muon-based signals and physics backgrounds are simulated starting from stopped muons recorded during the pileup simulation, which are resampled many times, forcing them to decay via a desired physics process. Stopped muons are plentiful in the pileup sample, and existing muon-based signal and physics background samples are many times larger than what Mu2e might observe in the region of interest. Cosmic ray backgrounds are simulated starting with standard generators (CRY, CORSIKA), stopping and resampling the particles entering the CRV. Current samples correspond to roughly 3 times the expected \runone sky live-time. The net output of the pileup, signal, and physics background simulations are energy deposits in the Mu2e detector. Simulated samples of raw on-spill data are created by mixing samples of pile-up and signal/physics background scaled to the expected beam intensity average and fluctuations. The different sources are finally mixed to produce a realistic sample, and processed with the same reconstruction code used to produce the data. 

Tools and policies are in place for code management, code review, building code, code validation, release management, release distribution, setup of the development environment, and setup of the runtime environment. For example, the code base is maintained in Git repositories, and the GitHub pull request system is used to control and review contributions to the code base. The SCD-supplied CMSBOT software is also used for the launch tests that are executed on the SCD Jenkins system. These tools will continue to evolve to integrate changes in the supported product stack by SCD and FIFE. Calibration data are stored in condition databases, indexed by (fraction of) runs, and grouped into intervals of validity. The metadata needed to access and process data (DQM, MetaCat, Rucio, luminosity,...) are stored in several dedicated databases. All databases are supported by the database group of the FNAL IT division. 

The analysis model is designed to be lightweight and flexible. Information about Mu2e events will be provided to users in the the form of ROOT ntuples containing a simplified output of the full reconstruction algorithms (while we expect the vast majority of analysts will use reduced data sets, analyses could still be conducted within the full framework). These reduced datasets require much less storage, facilitating data analysis on local resources and reducing the development time of analysis pipelines. A Python interface and a common analysis environment will also be provided to facilitate the inclusion of external tools (e.g. machine learning or statistical tools). In addition, several event displays are available to visually inspect individual events and help design analysis codes.

To this day, generators for all of the relevant signal and background processes exist and have been exercised in Mock Data campaigns. Reconstruction algorithms exist for all of the Mu2e detectors, with the majority of detector response functions tuned to measured data. Most of these algorithms are highly advanced and in nearly final form, where further optimization requires actual data. Many detector calibration algorithms have been demonstrated using bench test data and simulations. In many cases, these are fully developed, while in others they constitute only proof of principle. Some algorithms still require further development to meet the performance requirements. However, none of the missing calibrations are critical to triggering, recording, or evaluating the quality of Mu2e data, but will be required for precision physics analyses. 

Procedures to automatically process raw data as they appear from the online system exist and have been demonstrated to meet requirements, using data from bench tests and simulations. A conditions database and file catalog system based on the central tools currently supported by CSAID are in active, daily use, and have been exercised at scale in simulation campaigns. Operation of Geant4 in multithreaded mode on HPC resources has been demonstrated. The performance of the Offline processing has been benchmarked using preliminary estimates of the data volumes the Online system will produce, and shown to fit within the processing and storage envelope agreed with FNAL central computing. Configurations of the Offline algorithms run as part of the Online software-based event selection process (trigger) are in an advanced development stage and have been shown to meet requirements for \runone operations.

Code development adheres to industry standards and best practices, following a continuous integration (CI) model in which changes are frequently integrated into the source code. All changes are done via pull requests reviewed by experts and validated with a series of quality checks before integration. Database development is fully integrated with this workflow and synchronized with software releases. Prototype analysis interfaces to the Mu2e data have been developed and are in active daily use for developing analyses and calibration algorithms.