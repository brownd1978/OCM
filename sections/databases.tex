\section{Databases (Ray)}
\label{sec:databases}
\label{database}
The following is a list of Mu2e databases.
We do not include databases related to the Mu2e project, detector hardware, documents, or deprecated use cases. Data-handling databases are addessed in XXX.  The DQM database is discussed in XXX.

\begin{itemize}
  \item DAQ configuration (online, not streamed offline)
  \item run conditions database (online, streamed offline)
  \item EPICS archive database (online, streamed offline)
  \item conditions
  \item good run
  \item luminosity
\end{itemize}

\subsection{Infrastructure} \label{database-infrastructure}
In implementing database infrastructure, we are following lab recommendations and leveraging lab expertise and support.  The DAQ database is MongoDB, but all other databases are version 14 pr 15 postgres.  The postgres databases are all hosted by the lab database group.  This support provides the postgress software, the platform and servers. (Online database managed by the datbase group, but hosted on a Mu2e DAQ server.) All accounts are authenticated by kerberos principles and are created by the database support group via a servicedesk ticket.  The support group also provides the streaming service for copying from the online to offline instance.  Currently the stream is up-to-date within 30 min, but this can be reduced to 10 min when needed.

Offline, all database content is written by SQL commands generated by Mu2e tools, implemented using the psql exe at the command line, or libpq in c++ or psycopg2 in python.  All reads of the offline databases are though the Query Engine lab-supported product.  This system provides an http interface for all simple, high-rate database reads and is critical for providing conditions content to grid jobs.  Access is not authenticated, but is limited to on-site, VPN or select grid sites (XXX need to re-check).  There are two types of reads provided: reads from web server cache (the cache refreshed with a programmable lifetime), or directly from the database via the web server.  The service provides a valuable real-time monitor of system response times, including timelines.

Mu2e has been operating with these systems for several years and we expect they will meet all our needs.


\subsection{Run Conditions} \label{database-run}
The DAQ database is managed by the online operations group.  At each begin run, the DAQ copies all information that is used to configure the detector and DAQ for the run, from the MongoDB DAQ database into the online run conditions database.   Each run is divided into subruns (see XXX), and this system also records the precise times of the subruns in both wall clock time and in accelerator ticks.
At end-run, totals for events processed, run time, and other information is added to the record.

This content is streamed to an offline instance and is used several ways.  The first data reconstruction, and how to configure that reconstruction, will be triggered here, as a new runs appear.  The users can refer to a web page with interactive search for data discovery - what are recent runs, where are they configured, how many events to they contain.  Lastly, tools are provided to extract the trigger configuration so the trigger can be reproduced offline for investigations, or simulation. 

\subsection{EPICS} \label{database-epics}
EPICS is a system which can monitor, display and alarm of arbitrary quantities, such as voltages, status and environmental conditions. Online, this stream of monitoring data is archives in the online postgres database, using standard EPICS tools, and them streamed to the offline instance.  Offline, users can access content through a simple custom tool, epicsTool, which can select and report content.  This is available at the command line and in c++.

Some detector response is sensitive to environmental conditions.  If this information is required in reconstruction, it is automatically extracted from the EPICS database offline instance and re-packaged in the conditions system for reproducibility.

\subsection{Conditions} \label{database-conditions}
The Mu2e conditions database tracks the detector conditions (pedestals, gains, bad channels) and their run dependence.  The schema is custom, and has the following main features.  The user specifies a purpose (like ``Pass1'', ``Sim2020'' or "CaloCalibration''.  A version number, together with the purpose, defines what ``conditions set'' the job will access. The version number has three fields: the major version is used by experts to alert the user to an important change in conditions philosophy, the minor number indicates changes to content, and the final number indicates the extension to list the intervals of validity.  If the major and minor number are the same, the jobs which ran before will run the same. The schema implements a closed interval of validity (with an option to relax if needed).  All conditions access is reproducible for all time, as a design requirement.  If a repair is needed, or new types of tables need to be added, a new minor version is created.

Implementation features include a text table option for overriding database content for tests, or to test new tables before they ae part of the database schema.  In extraordinary circumstance when database access is not available, the text option can provide the entire conditions set content. A programmable onboard cache saves tables so switching back and forth between tables does not generate new fetches.  All external operations are timed and we find the per-table-fetch time is the slowest component.  Multiple layers of verbosity are provided to help debugging.  The code is designed to be thread-safe, but has not been thoroughly tested in this regard.

Wrapping the database access is layer of code called Proditions.  This layer provides the following features.  The user interface can be different from the raw database tables, for example, passing the user structures or subsets of table data.  This layer also allows intermediate calculations based on the database contents, and the run-dependence of this content is automatically tracked based on the database table run dependence.  Proditions also has a onboard cache to prevent thrashing.

A development database exists with the identical schema as the production database. When a new calibration table is proposed, it is first installed in the development database to verify the SQL commands and compatibility with code, then the same table is created in production.  Other operations can be also tested first in development.  We have not found the need for an integration database yet, but if one is needed, it will be straightforward to create.

Each subdetector defines what tables they need and they are added to the schema. The users may create ``archive'' tables to store intermediate results which are no used in reconstruction, but should still be saved.  The system also provides ``ad-hoc'' tables for secondary needs, like record-keeping. In operations, the subdetector expert provides content and IoV's, and passes them to the calibration coordinator, who joins the subdetector content together into a conditions set.

All tools needed for database upload, readback and repair exist.  The conditions database system has been in daily use for several years.  The system has been informally load-tested, and will greatly exceed requirements for pass1, the most time-sensitive operation.  A more formal load test will be coupled to raw data upload and pass1.

\subsection{Goodrun} \label{database-goodrun}
Different analysis groups may define what is usable data differently and should have the ability to define what runs they want to use for a a purpose.  What is considered good may also involve input from many subdetector and other experts. The good-run database collects input from experts, as a status (typically an int) for a given IoV.  The user of these, then can define a selection, of these reports, add information from run conditions, adjust the selection by hand, then preserve the selection in the database.  Access to the good run list is by tools, or as a filter in processing.

\subsection{Luminosity} \label{database-luminosity}
The number of stopped muons is the denominator in the measurement or limit for charge lepton flavor violation, the primary goal of Mu2e.  Towards this end, it may be useful to track the number of protons on the primary target, possibly as frequently as every bunch.  We will track several quantities per proton bunch, resulting in a large dataset.  We expect this will be preserved in a database, and accessed using custom tools, as an efficiency-weighted luminosity independent, of event processing on the grid. (XXX this section is weak)
